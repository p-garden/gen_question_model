{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0f9964-65eb-41c5-8c81-29580b876e25",
   "metadata": {},
   "source": [
    "- T5ëª¨ë¸ (Text-to-Text Transfer Transformer)\n",
    "    - ì…ë ¥í…ìŠ¤íŠ¸, íƒœìŠ¤í¬ì •ì˜ ë¥¼ ì…ë ¥í•˜ë©´ íƒœìŠ¤í¬ì— ë§ê²Œ ì…ë ¥í…ìŠ¤íŠ¸ë¡œë¶€í„° ë™ì‘ì„ ìˆ˜í–‰\n",
    "    - ë¬¸ì œ ìƒì„±, ì˜¤ë‹µì„ ì§€ ìƒì„±, ì •ë‹µì°¾ê¸°ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ êµ¬ì¶• ê°€ëŠ¥\n",
    "\n",
    "ì…ë ¥ ë°ì´í„° í˜•ì‹(í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼):\n",
    "\n",
    " JSONí˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸, ë¬¸ì¥, í‚¤ì›Œë“œë“±ì„ í¬í•¨í•œ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ì…ë ¥ë°›ê¸°\n",
    "\n",
    "ì¶œë ¥ ë°ì´í„° í˜•ì‹: \n",
    "\n",
    "ì¶œì œëœ ë¬¸ì œì™€ ìë£Œí† ëŒ€ë¡œí•œ ì •ë‹µ\n",
    "\n",
    "### **ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ í™œìš© + Few-shot Learning ì „ëµ**\n",
    "\n",
    "T5ëª¨ë¸ì„ ì‚¬ìš© (ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í†µì¼)\n",
    "\n",
    "```json\n",
    "Task: [ìˆ˜í–‰í•  ì‘ì—…] Input: [ì²˜ë¦¬í•  í…ìŠ¤íŠ¸]\n",
    "\t[ì‘ì—…]\n",
    "\t\"generate question:\" â†’ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ ìƒì„±.\n",
    "\t\"summarize:\" â†’ í…ìŠ¤íŠ¸ ìš”ì•½.\n",
    "\t\"translate English to French:\" â†’ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í”„ë‘ìŠ¤ì–´ë¡œ ë²ˆì—­.\n",
    "\t\"extract answer:\" â†’ ì§€ë¬¸ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µ ì¶”ì¶œ.\n",
    "\n",
    "```\n",
    "\n",
    "â†’ì´ë¯¸ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ / í† í°í™”ë“± ë°ì´í„° ì „ì²˜ë¦¬ í•„ìš”X (ì¼ë°˜ì ì¸ ì „ì²˜ë¦¬ëŠ” T5ëª¨ë¸ ë‚´ë¶€ì—ì„œ ì‹¤í–‰ íŠ¹ìˆ˜ë¬¸ì ì œê±°, ìŠ¬ë¼ì´ì‹±, ë„ë©”ì¸íŠ¹í™” ë“±ë“± íŠ¹ìˆ˜í•œ ì „ì²˜ë¦¬ë§Œ ì‹¤í–‰)\n",
    "\n",
    "ì´ˆê¸° ëª¨ë¸ë§ì‹œ ëª‡ê°œì˜ PDfíŒŒì¼ë¡œ FineTuning\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": \"generate question: The Eiffel Tower was completed in 1889.\",\n",
    "  \"output\": \"When was the Eiffel Tower completed?\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bbe05-0129-490f-bb37-0d6ec2452066",
   "metadata": {},
   "source": [
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d442f1-b0ec-49f1-bfcb-43c4fa8954da",
   "metadata": {},
   "source": [
    "ê°€ìƒí™˜ê²½: cd C:\\Users\\j2982\\gen_question_env  \r",
    "    gen_question_env\\Scripts\\activate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f050b345-7382-4b7e-af7b-282ccd29e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a2ed12-00f7-494f-b4ec-a6466a1aa951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78c2fef-e38b-4c64-91ee-b88ad388a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WINDOWS\\system32\\gen_que_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb42328-7eea-48dd-a6cb-7e950bad3434",
   "metadata": {},
   "source": [
    "# 2. ëª¨ë¸, í† í¬ë‚˜ì´ì € ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cfcc80-d3b7-471d-ae3d-8441d3c37209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b734d7a-c0f8-45f2-89a5-20d943f60e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12225819-ed59-4d61-b657-81ae24dd152f",
   "metadata": {},
   "source": [
    "# 3. ë°ì´í„°ì¤€ë¹„(Fiine Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27cfd05-08f3-4c1f-b4af-b3fb668afeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8029e7b9-1773-4240-84fa-4fab21c02b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afefcd1c-b5ee-4817-a311-12db3c4bd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate==0.30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60623638-9900-4d15-8b71-4f50def3ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27.2\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994c997e-cfed-4174-b417-3c3395722aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (4.48.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\j2982\\anaconda 3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bffbd8b-b95f-4d1a-8da7-b4048cf7588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.48.0\n",
      "Accelerate Version: 0.27.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "print(\"Accelerate Version:\", accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f69c74-75a1-4a00-9330-6481f1b8e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    {\n",
    "        \"input\": \"generate question: The Eiffel Tower was completed in 1889.\",\n",
    "        \"output\": \"When was the Eiffel Tower completed?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"generate question: The Great Wall of China is over 13,000 miles long.\",\n",
    "        \"output\": \"How long is the Great Wall of China?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"generate distractors: 1889 in the context of 'The Eiffel Tower was completed in 1889.'\",\n",
    "        \"output\": \"1789, 1905, 1923\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb9747e-2e0f-4e78-823d-f7126ead7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë³€í™˜\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in train_data],\n",
    "    \"output\": [item[\"output\"] for item in train_data]\n",
    "})\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "valid_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d034044-a3c1-4ae9-be6b-ca7923a6ce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502f9f30-9e12-496a-82c5-a55f3cb6b345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 32.79 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# í† í°í™” í•¨ìˆ˜\n",
    "def tokenize_function(batch):\n",
    "    inputs = tokenizer(batch[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = tokenizer(batch[\"output\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# ë°ì´í„°ì…‹ í† í°í™”\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_valid = valid_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44834e1a-a6a2-46b6-ba45-b451be3fe591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WINDOWS\\system32\\gen_que_env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\j2982\\AppData\\Local\\Temp\\ipykernel_24496\\2093866170.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_finetuned\",\n",
    "    evaluation_strategy=\"steps\",  # í‰ê°€ë¥¼ steps ë‹¨ìœ„ë¡œ ì„¤ì •\n",
    "    save_strategy=\"steps\",        # ëª¨ë¸ ì €ì¥ë„ steps ë‹¨ìœ„ë¡œ ì„¤ì •\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,               # ì €ì¥ ê°„ê²©\n",
    "    eval_steps=500,               # í‰ê°€ ê°„ê²©\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,  # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ë¡œë“œ\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Trainer ê°ì²´ ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c44afe-3883-4d81-a595-f47c8893411c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=15.758079528808594, metrics={'train_runtime': 213.9412, 'train_samples_per_second': 0.028, 'train_steps_per_second': 0.014, 'total_flos': 3653747343360.0, 'train_loss': 15.758079528808594, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuning ì‹¤í–‰\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab9d8ec-7bbd-40b2-8a69-9d344a023ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5_finetuned_model\\\\tokenizer_config.json',\n",
       " './t5_finetuned_model\\\\special_tokens_map.json',\n",
       " './t5_finetuned_model\\\\spiece.model',\n",
       " './t5_finetuned_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./t5_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./t5_finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "494c8cff-fc1b-4a8b-b1ae-14af0ab192db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: True\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "test_input = \"generate question: The Eiffel Tower was completed in 1889.\"\n",
    "input_ids = tokenizer(test_input, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# ëª¨ë¸ ì¶”ë¡ \n",
    "outputs = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "print(\"Generated Question:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dd81e-f92e-4924-8524-238d8a07fe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gen_que_env)",
   "language": "python",
   "name": "gen_que_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
