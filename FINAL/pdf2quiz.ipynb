{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설치 (필요시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### poppler-utils 설치 (필요시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Poppler 홈페이지](https://poppler.freedesktop.org/)를 통해 다운로드 혹은 아래 코드를 이용 <span style=\"opacity: 0.5;\">(아래 코드 이용시 윈도우의 경우 Chocolatey 설치 필요)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def is_conda():\n",
    "    return \"conda\" in sys.prefix or os.path.exists(os.path.join(sys.prefix, \"conda-meta\"))\n",
    "\n",
    "os_name = platform.system()\n",
    "\n",
    "if is_conda():\n",
    "    os.system(\"conda install conda-forge::poppler -y\")\n",
    "elif os_name == \"Windows\":\n",
    "    os.system(\"choco install poppler-utils -y\")\n",
    "elif os_name == \"Linux\":\n",
    "    os.system(\"apt-get update && apt-get install -y poppler-utils\")\n",
    "elif os_name == \"Darwin\":\n",
    "    os.system(\"brew install poppler\")\n",
    "else:\n",
    "    print(\"지원되지 않는 OS이니 수동으로 poppler를 설치하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "from doclayout_yolo import YOLOv10\n",
    "from pdf2image import convert_from_path\n",
    "from konlpy.tag import Okt\n",
    "from keybert import KeyBERT\n",
    "import kss\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import Pix2StructForConditionalGeneration, AutoProcessor, AutoModelForSeq2SeqLM, AutoTokenizer, BertTokenizer, BertForNextSentencePrediction, BartForConditionalGeneration, PreTrainedTokenizerFast, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 페이지 레이아웃 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PDF -> jpg 꼴로 페이지 단위 이미지로 변환 (poppler 사용)\n",
    "2. YOLOv10 모델을 이용하여 영역 검출\n",
    "3. IOU를 계산하여 겹치는 부분 등의 문제 해결\n",
    "4. 각 구분마다 이미지를 크롭하고 메타데이터와 함께 이를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# PDF 파일을 페이지별 이미지(PIL Image)로 변환하는 함수\n",
    "def pdf_to_images(pdf_path, dpi=300):\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "    print(f\"Converted PDF to {len(images)} page objects.\")\n",
    "    return images\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# YOLOv10 모델 로드 함수 (DocLayout-YOLO)\n",
    "def load_yolo_model():\n",
    "    filepath = hf_hub_download(\n",
    "        repo_id=\"juliozhao/DocLayout-YOLO-DocStructBench\",\n",
    "        filename=\"doclayout_yolo_docstructbench_imgsz1024.pt\"\n",
    "    )\n",
    "    return YOLOv10(filepath)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 두 박스 간 IoU 계산 함수\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "    inter_x1 = max(x1, x3)\n",
    "    inter_y1 = max(y1, y3)\n",
    "    inter_x2 = min(x2, x4)\n",
    "    inter_y2 = min(y2, y4)\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x4 - x3) * (y4 - y3)\n",
    "    return inter_area / (box1_area + box2_area - inter_area)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 중복 박스 제거 함수 (IoU 기준)\n",
    "def filter_duplicate_boxes(bounding_boxes, iou_threshold=0.5):\n",
    "    filtered_boxes = []\n",
    "    for box in bounding_boxes:\n",
    "        keep = True\n",
    "        for fbox in filtered_boxes:\n",
    "            iou = calculate_iou(\n",
    "                (box[\"x_min\"], box[\"y_min\"], box[\"x_max\"], box[\"y_max\"]),\n",
    "                (fbox[\"x_min\"], fbox[\"y_min\"], fbox[\"x_max\"], fbox[\"y_max\"])\n",
    "            )\n",
    "            if iou > iou_threshold:\n",
    "                if box[\"confidence\"] > fbox[\"confidence\"]:\n",
    "                    filtered_boxes.remove(fbox)\n",
    "                else:\n",
    "                    keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            filtered_boxes.append(box)\n",
    "    return filtered_boxes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 고유 접미사 생성 (식별용)\n",
    "def generate_unique_suffix(index):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    return alphabet[index % len(alphabet)]\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 단일 이미지(페이지)에서 영역(박스) 검출 함수\n",
    "def process_image(image, model, page_number):\n",
    "    image_array = np.array(image)\n",
    "    det_res = model.predict(image_array, imgsz=1024, conf=0.2, device=DEVICE)\n",
    "    bounding_boxes = []\n",
    "    for i, box in enumerate(det_res[0].boxes):\n",
    "        class_name = model.names[int(box.cls)]\n",
    "        class_number = int(box.cls)\n",
    "        unique_suffix = generate_unique_suffix(i)\n",
    "        bounding_boxes.append({\n",
    "            \"class\": class_name,\n",
    "            \"confidence\": float(box.conf),\n",
    "            \"x_min\": float(box.xyxy[0][0]),\n",
    "            \"y_min\": float(box.xyxy[0][1]),\n",
    "            \"x_max\": float(box.xyxy[0][2]),\n",
    "            \"y_max\": float(box.xyxy[0][3]),\n",
    "            \"unique_id\": f\"page{page_number}_class{class_number}_{unique_suffix}\",\n",
    "            \"page_number\": page_number\n",
    "        })\n",
    "    filtered_boxes = filter_duplicate_boxes(bounding_boxes, iou_threshold=0.5)\n",
    "    return filtered_boxes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PDF 전체에 대해 페이지별 이미지 변환 및 영역 검출\n",
    "def process_pdf(pdf_path, model, dpi=300):\n",
    "    images = pdf_to_images(pdf_path, dpi=dpi)\n",
    "    all_detections = []\n",
    "    for page_number, image in enumerate(images, start=1):\n",
    "        detections = process_image(image, model, page_number)\n",
    "        all_detections.append(detections)\n",
    "    return images, all_detections\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 검출된 영역을 원본 이미지에서 크롭하고, bounding_box 및 page_number 정보를 포함해 분리\n",
    "def crop_detections(images, all_detections):\n",
    "    cropped_results = {\"table\": [], \"plain text\": [], \"figure\": []}\n",
    "    for detections in all_detections:\n",
    "        if not detections:\n",
    "            continue\n",
    "        page_number = detections[0][\"page_number\"]\n",
    "        image = np.array(images[page_number - 1])\n",
    "        for box in detections:\n",
    "            x_min = int(box[\"x_min\"])\n",
    "            y_min = int(box[\"y_min\"])\n",
    "            x_max = int(box[\"x_max\"])\n",
    "            y_max = int(box[\"y_max\"])\n",
    "            cropped_img = image[y_min:y_max, x_min:x_max]\n",
    "            category = box[\"class\"]\n",
    "            region_dict = {\n",
    "                \"unique_id\": box[\"unique_id\"],\n",
    "                \"image\": cropped_img,\n",
    "                \"page_number\": page_number,\n",
    "                \"bounding_box\": {\n",
    "                    \"x_min\": box[\"x_min\"],\n",
    "                    \"y_min\": box[\"y_min\"],\n",
    "                    \"x_max\": box[\"x_max\"],\n",
    "                    \"y_max\": box[\"y_max\"]\n",
    "                }\n",
    "            }\n",
    "            if category in cropped_results:\n",
    "                cropped_results[category].append(region_dict)\n",
    "            else:\n",
    "                if \"other\" not in cropped_results:\n",
    "                    cropped_results[\"other\"] = []\n",
    "                cropped_results[\"other\"].append(region_dict)\n",
    "    return cropped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 평문 영역에서의 텍스트 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EasyOCR 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 평문 영역에서 EasyOCR으로 텍스트 추출 (EasyOCR 사용)\n",
    "class TextExtractorFromMemory:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reader = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())\n",
    "        \n",
    "    def extract_text(self, image):\n",
    "        text_result = self.reader.readtext(image, detail=0)\n",
    "        text = \" \".join(text_result).strip()\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 평문 영역 처리 함수: 최종 출력 구조는 아래와 같이 함\n",
    "# {data_id, page_number, region_type, content, meta:{bounding_box: ...}}\n",
    "def process_plain_text_regions(plain_text_regions):\n",
    "    extractor = TextExtractorFromMemory()\n",
    "    results = []\n",
    "    for region in plain_text_regions:\n",
    "        unique_id = region[\"unique_id\"]\n",
    "        text = extractor.extract_text(region[\"image\"])\n",
    "        results.append({\n",
    "            \"data_id\": unique_id,\n",
    "            \"page_number\": region[\"page_number\"],\n",
    "            \"region_type\": \"평문\",\n",
    "            \"content\": text,\n",
    "            \"meta\": {\n",
    "                \"bounding_box\": region[\"bounding_box\"]\n",
    "            }\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 표 영역의 텍스트 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV와 EasyOCR을 기반으로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 표 영역 처리 함수: 표의 셀 텍스트 및 그리드 정보 추출\n",
    "def extract_text_from_cells(cells_data):\n",
    "    extracted_text = []\n",
    "    for cell in cells_data:\n",
    "        if 'text' in cell:\n",
    "            extracted_text.append(cell['text'])\n",
    "    return ' '.join(extracted_text)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 표 영역에서 표 구조 및 셀 텍스트 추출 (OpenCV와 EasyOCR 사용)\n",
    "class TableExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reader = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())\n",
    "\n",
    "    def process_image(self, image):\n",
    "        if isinstance(image, str):\n",
    "            self.image = cv2.imread(image)\n",
    "        else:\n",
    "            self.image = image\n",
    "        self.result = self.image.copy()\n",
    "        self.detect_lines()\n",
    "        self.classify_lines_and_find_intersections()\n",
    "        self.remove_duplicate_points()\n",
    "        data, extracted_cells = self.extract_text_from_cells()\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        df = df.replace('', np.nan)\n",
    "        df = df.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df.fillna('')\n",
    "        processed_cells = []\n",
    "        for i in range(len(df)):\n",
    "            for j in range(len(df.columns)):\n",
    "                original_cell = next((cell for cell in extracted_cells if cell['row'] == i + 1 and cell['col'] == j + 1), None)\n",
    "                if original_cell:\n",
    "                    processed_cells.append({\n",
    "                        'row': i + 1,\n",
    "                        'col': j + 1,\n",
    "                        'text': df.iloc[i, j],\n",
    "                        'coordinates': original_cell['coordinates']\n",
    "                    })\n",
    "        final_result = {'cells': processed_cells, 'grid_info': {'rows': len(df), 'cols': len(df.columns)}}\n",
    "        return final_result\n",
    "    \n",
    "    def detect_lines(self):\n",
    "        self.edges = cv2.Canny(self.image, 50, 150, apertureSize=3)\n",
    "        self.lines = cv2.HoughLinesP(self.edges, 1, np.pi/180, threshold=100, minLineLength=100, maxLineGap=10)\n",
    "        return self.lines\n",
    "    \n",
    "    def classify_lines_and_find_intersections(self):\n",
    "        self.intersection_points = []\n",
    "        self.horizontal_lines = []\n",
    "        self.vertical_lines = []\n",
    "        if self.lines is not None:\n",
    "            for line in self.lines:\n",
    "                x1, y1, x2, y2 = line[0]\n",
    "                angle = np.abs(np.arctan2(y2 - y1, x2 - x1) * 180.0 / np.pi)\n",
    "                if angle < 10 or angle > 170:\n",
    "                    self.horizontal_lines.append(line[0])\n",
    "                elif 80 < angle < 100:\n",
    "                    self.vertical_lines.append(line[0])\n",
    "            height, width = self.image.shape[:2]\n",
    "            margin = 10\n",
    "            self.horizontal_lines.append([margin, margin, width - margin, margin])\n",
    "            self.horizontal_lines.append([margin, height - margin, width - margin, height - margin])\n",
    "            self.vertical_lines.append([margin, margin, margin, height - margin])\n",
    "            self.vertical_lines.append([width - margin, margin, width - margin, height - margin])\n",
    "            self._find_intersection_points()\n",
    "            self._process_end_points()\n",
    "\n",
    "    def _find_intersection_points(self):\n",
    "        for h_line in self.horizontal_lines:\n",
    "            for v_line in self.vertical_lines:\n",
    "                x1, y1, x2, y2 = h_line\n",
    "                x3, y3, x4, y4 = v_line\n",
    "                denominator = ((x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4))\n",
    "                if denominator != 0:\n",
    "                    t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denominator\n",
    "                    u = -((x1 - x2) * (y1 - y3) - (y1 - y2) * (x1 - x3)) / denominator\n",
    "                    if 0 <= t <= 1 and 0 <= u <= 1:\n",
    "                        x = int(x1 + t * (x2 - x1))\n",
    "                        y = int(y1 + t * (y2 - y1))\n",
    "                        self.intersection_points.append((x, y))\n",
    "        self.intersection_points = sorted(set(self.intersection_points), key=lambda p: (p[1], p[0]))\n",
    "\n",
    "    def _process_end_points(self):\n",
    "        end_points = []\n",
    "        for line in self.horizontal_lines + self.vertical_lines:\n",
    "            x1, y1, x2, y2 = line\n",
    "            end_points.append((x1, y1))\n",
    "            end_points.append((x2, y2))\n",
    "        x_values = [point[0] for point in end_points]\n",
    "        y_values = [point[1] for point in end_points]\n",
    "        x_min, x_max = min(x_values), max(x_values)\n",
    "        y_min, y_max = min(y_values), max(y_values)\n",
    "        self.filtered_end_points = [(x, y) for (x, y) in end_points if (x_min <= x <= x_min + 10 or x_max - 10 <= x <= x_max) or (y_min <= y <= y_min + 10 or y_max - 10 <= y <= y_max)]\n",
    "        self.all_points = self.intersection_points + self.filtered_end_points\n",
    "\n",
    "    def remove_duplicate_points(self, distance_threshold=15):\n",
    "        self.unique_points = []\n",
    "        for point in self.all_points:\n",
    "            is_unique = True\n",
    "            for unique_point in self.unique_points:\n",
    "                distance = np.linalg.norm(np.array(point) - np.array(unique_point))\n",
    "                if distance <= distance_threshold:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                self.unique_points.append(point)\n",
    "\n",
    "    def extract_text_from_cells(self, min_height=30, min_width=30):\n",
    "        self.x_coords = sorted(list(set([point[0] for point in self.intersection_points])))\n",
    "        self.y_coords = sorted(list(set([point[1] for point in self.intersection_points])))\n",
    "        data = []\n",
    "        extracted_cells = []\n",
    "        for i in range(len(self.y_coords) - 1):\n",
    "            row = []\n",
    "            for j in range(len(self.x_coords) - 1):\n",
    "                top_left_x = self.x_coords[j]\n",
    "                top_left_y = self.y_coords[i]\n",
    "                bottom_right_x = self.x_coords[j + 1]\n",
    "                bottom_right_y = self.y_coords[i + 1]\n",
    "                tile = self.image[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "                cell_info = {'row': i + 1, 'col': j + 1, 'coordinates': {'top_left': (top_left_x, top_left_y), 'bottom_right': (bottom_right_x, bottom_right_y)}}\n",
    "                if tile.shape[0] < min_height or tile.shape[1] < min_width:\n",
    "                    row.append(\"\")\n",
    "                    cell_info['text'] = \"\"\n",
    "                    extracted_cells.append(cell_info)\n",
    "                    continue\n",
    "                text_result = self.reader.readtext(tile, detail=0)\n",
    "                text = \"\\n\".join(text_result).strip()\n",
    "                row.append(text)\n",
    "                cell_info['text'] = text\n",
    "                extracted_cells.append(cell_info)\n",
    "            data.append(row)\n",
    "        return data, extracted_cells\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 표 영역 처리 함수: 최종 출력 구조에 data_id, page_number, region_type, content, meta 포함\n",
    "def process_table_regions(table_regions):\n",
    "    table_extractor = TableExtractor()\n",
    "    results = []\n",
    "    for region in table_regions:\n",
    "        unique_id = region[\"unique_id\"]\n",
    "        try:\n",
    "            table_result = table_extractor.process_image(region[\"image\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Table extraction failed for {unique_id}: {e}\")\n",
    "            continue\n",
    "        table_text = extract_text_from_cells(table_result[\"cells\"])\n",
    "        results.append({\n",
    "            \"data_id\": unique_id,\n",
    "            \"page_number\": region[\"page_number\"],\n",
    "            \"region_type\": \"일반표\",\n",
    "            \"content\": table_text,\n",
    "            \"meta\": {\n",
    "                \"bounding_box\": region[\"bounding_box\"],\n",
    "                \"cells\": table_result.get(\"cells\", []),\n",
    "                \"grid\": table_result.get(\"grid_info\", {})\n",
    "            }\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 도표 영역 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2Struct를 기반하여 이를 사전 학습시킨 brainventures/deplot_kr을 불러와서 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 도표 영역 처리: Pix2Struct를 사용하여 도표 설명 생성\n",
    "class FigureExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processor = AutoProcessor.from_pretrained(\"brainventures/deplot_kr\")\n",
    "        self.model = Pix2StructForConditionalGeneration.from_pretrained(\"brainventures/deplot_kr\")\n",
    "        self.model.to(DEVICE)\n",
    "        \n",
    "    def extract_figure_info(self, image):\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "        outputs = self.model.generate(**inputs, max_length=1024)\n",
    "        result = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        return result\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 도표 영역 처리 함수: 최종 출력 구조에 data_id, page_number, region_type, content, meta 포함\n",
    "def process_figure_regions(figure_regions):\n",
    "    figure_extractor = FigureExtractor()\n",
    "    results = []\n",
    "    for region in figure_regions:\n",
    "        unique_id = region[\"unique_id\"]\n",
    "        try:\n",
    "            figure_text = figure_extractor.extract_figure_info(region[\"image\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Figure extraction failed for {unique_id}: {e}\")\n",
    "            continue\n",
    "        results.append({\n",
    "            \"data_id\": unique_id,\n",
    "            \"page_number\": region[\"page_number\"],\n",
    "            \"region_type\": \"도표\",\n",
    "            \"content\": figure_text,\n",
    "            \"meta\": {\n",
    "                \"bounding_box\": region[\"bounding_box\"]\n",
    "            }\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# pdf를 json으로 변환하는 함수\n",
    "def pdf2json():\n",
    "    # 1. PDF 선택\n",
    "    pdf_folder = \"./\"\n",
    "    pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"PDF 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"\\n찾은 PDF 목록:\")\n",
    "        for i, pdf in enumerate(pdf_files, start=1):\n",
    "            print(f\"\\t{i}. {os.path.basename(pdf)}\")\n",
    "        while True:\n",
    "            try:\n",
    "                choice = int(input(\"\\n사용할 PDF 번호를 입력하세요: \"))\n",
    "                if 1 <= choice <= len(pdf_files):\n",
    "                    pdf_path = pdf_files[choice - 1]\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"유효한 번호를 입력하세요.\")\n",
    "            except ValueError:\n",
    "                print(\"숫자만 입력하세요.\")\n",
    "    # 2. 모델 로드, PDF→이미지 변환 및 영역 검출\n",
    "    model = load_yolo_model()\n",
    "    images, all_detections = process_pdf(pdf_path, model, dpi=300)\n",
    "    cropped_results = crop_detections(images, all_detections)\n",
    "    # 3. 영역별(평문, 표, 도표)로 분리\n",
    "    plain_text_regions = cropped_results.get(\"plain text\", [])\n",
    "    table_regions = cropped_results.get(\"table\", [])\n",
    "    figure_regions = cropped_results.get(\"figure\", [])\n",
    "    print(f\"Total plain text regions: {len(plain_text_regions)}\")\n",
    "    print(f\"Total table regions: {len(table_regions)}\")\n",
    "    print(f\"Total figure regions: {len(figure_regions)}\")\n",
    "    # 4. 각 영역별 텍스트 추출 및 처리\n",
    "    plain_text_extraction_results = process_plain_text_regions(plain_text_regions)\n",
    "    table_extraction_results = process_table_regions(table_regions)\n",
    "    figure_extraction_results = process_figure_regions(figure_regions)\n",
    "    # 5. 세 영역 결과를 합침\n",
    "    combined_results = plain_text_extraction_results + table_extraction_results + figure_extraction_results\n",
    "    # 6. 사람이 읽는 순서대로 정렬 (bounding_box의 y_min, x_min 순)\n",
    "    combined_results = sorted(combined_results, key=lambda x: (x[\"page_number\"], x[\"meta\"][\"bounding_box\"][\"y_min\"], x[\"meta\"][\"bounding_box\"][\"x_min\"]))\n",
    "    # 7. 최종 결과를 JSON 파일로 저장\n",
    "    RESULTS_DIR = \"./processing data/PDF2JSON\"\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    final_combined_json = os.path.join(RESULTS_DIR, \"result.json\")\n",
    "    with open(final_combined_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Combined JSON extraction results saved to: {final_combined_json}\")\n",
    "    torch.cuda.empty_cache()  # GPU 캐시 메모리 해제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평문으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 그래프 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 그래프 데이터를 자연어 문장으로 변환하는 함수\n",
    "def plot_to_text(data):\n",
    "\n",
    "    def process_plot(plot):\n",
    "        contents = plot.content.split(\"\\n\") # 내용만 불러와서 \"\\n\"에 따라 분리\n",
    "        values = [line.split(\"|\") for line in contents if \"|\" in line] # 값 추출 (연도 | 값 형태)\n",
    "        text_description = \"대한민국 명목 국내 총 생산 연도별 값은 다음과 같다: \"\n",
    "        for year, value in values[1:]: # 데이터 변환\n",
    "            text_description += f\"{year.strip()}년에는 {value.strip()}(억)달러, \"\n",
    "        text_description = text_description.rstrip(\", \") + \"이다.\"\n",
    "        return text_description\n",
    "\n",
    "    data['content'] = data.apply(lambda x : process_plot(x) if (x.region_type == \"도표\") and (x.page_number != 1) else x.content, axis = 1)\n",
    "    data['region_type'] = data.apply(lambda x : \"평문\" if (x.region_type == \"도표\") and (x.page_number != 1) else x.region_type, axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 표 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 표 데이터를 자연어 문장으로 변환하는 함수\n",
    "def table_to_text(data):\n",
    "\n",
    "    def process_table(table):\n",
    "        structured_data = {}\n",
    "        cells = table[\"meta\"][\"cells\"]\n",
    "        categories = [\"정의\", \"계산 방식\", \"특징\"]\n",
    "        headers = []\n",
    "        for cell in cells: # 첫 번째 행에서 컬럼 헤더(제목) 추출\n",
    "            if cell[\"row\"] == 1:\n",
    "                headers.append(cell[\"text\"].strip())\n",
    "        num_columns = len(headers)\n",
    "        for cell in cells: # 데이터 추출 및 구조화\n",
    "            row, col = cell[\"row\"], cell[\"col\"]\n",
    "            text = cell[\"text\"].replace(\"\\n\", \" \").strip()  # 개행 제거 및 공백 정리\n",
    "            if row in [2, 3, 4]:  # '정의', '계산 방식', '특징' 카테고리에 해당하는 데이터만 추출\n",
    "                category = categories[row - 2]\n",
    "                if col - 1 < num_columns:\n",
    "                    column_name = headers[col - 1]\n",
    "                    if column_name not in structured_data:\n",
    "                        structured_data[column_name] = {}\n",
    "                    structured_data[column_name][category] = text\n",
    "        formatted_sentences = []\n",
    "        for column_name, content in structured_data.items(): # 구조화된 데이터를 문장으로 변환\n",
    "            definition = content.get(\"정의\", \"\")\n",
    "            calculation = content.get(\"계산 방식\", \"\")\n",
    "            characteristic = content.get(\"특징\", \"\")\n",
    "            sentence_parts = [\n",
    "                f\"{column_name}의 정의는 {definition}\" if definition else \"\",\n",
    "                f\"계산 방식은 {calculation}이며\" if calculation else \"\",\n",
    "                f\"특징은 {characteristic}\" if characteristic else \"\",\n",
    "            ]\n",
    "            formatted_sentence = \" \".join([part for part in sentence_parts if part]) + \".\"\n",
    "            formatted_sentences.append(formatted_sentence)\n",
    "        return formatted_sentences[1:]  # 첫 번째 요소 제거 (필요 없으면 수정 가능)\n",
    "    \n",
    "    def correct_text(data):\n",
    "        corrections = {\n",
    "            \"한 국가의 경제 규모름 축정 하는 대표적인 지표이다:.\": \"한 국가의 경제 규모를 측정하는 대표적인 지표이다.\",\n",
    "            \"한 나라의 국민이 국내외에 서 생산한 최종 생산물의 총 가치\": \"한 나라의 국민이 국내외에서 생산한 최종 생산물의 총 가치\",\n",
    "            \"국내충생산에서 해외에서 번 내국인의 소득올 더하고 외 국인이 국내에서 번 소득올 제외\": \"국내 총생산에서 해외에서 번 내국인의 소득을 더하고 외국인이 국내에서 번 소득을 제외\",\n",
    "            \"국민의 종소득 개념올 포함 하여 국제적 비교에 유리하 다.\": \"국민의 종소득 개념을 포함하여 국제적 비교에 유리하다.\",\n",
    "            \"국민종생산에서 감가상각올 제외한 값\": \"국민 총생산에서 감가상각을 제외한 값\",\n",
    "            \"국내총생산에서 감가상각올 제외\": \"국내총생산에서 감가상각을 제외\",\n",
    "            \"국민의 종소득 개념을 포함하여\" : \"국민의 총소득 개념을 포함하여\",\n",
    "            \"국내에서 실질적으로 창출캐 부가가치틀 파악하는데 유용 하다.\": \"국내에서 실질적으로 창출된 부가가치를 파악하는데 유용하다.\",\n",
    "            \"국민종생산에서 감가상각올 제외\": \"국민총생산에서 감가상각을 제외\",\n",
    "            \"국가의 실질적인 경제력올 파악하는데 유용하다.\": \"국가의 실질적인 경제력을 파악하는데 유용하다.\"\n",
    "        }\n",
    "        if isinstance(data, list):  # 리스트일 경우, 각 요소에 대해 재귀적으로 처리\n",
    "            return [correct_text(item) for item in data]\n",
    "        elif isinstance(data, str):  # 문자열일 경우, 오타 교정 수행\n",
    "            for wrong_text, correct_text_value in corrections.items():\n",
    "                data = data.replace(wrong_text, correct_text_value)\n",
    "            return data\n",
    "        else:\n",
    "            return data  # 문자열이 아닐 경우 원본 그대로 반환\n",
    "\n",
    "    data['content'] = data.apply(lambda x : \" \".join(correct_text(process_table(x))) if (x.region_type == \"일반표\") and ((x.meta['bounding_box']['y_max'])-(x.meta['bounding_box']['y_min']) > 500) else x.content, axis = 1)\n",
    "    data['region_type'] = data.apply(lambda x : \"평문\" if (x.region_type == \"일반표\") and ((x.meta['bounding_box']['y_max'])-(x.meta['bounding_box']['y_min']) > 500) else x.region_type, axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 텍스트 교정하는 함수\n",
    "def spell_correction(data):\n",
    "    data = data.loc[data.region_type == \"평문\"].reset_index(drop=True)\n",
    "    corpus_path = \"./processing data/corpora.json\"\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f: # 말뭉치 데이터 로드\n",
    "        corpus_data = json.load(f)\n",
    "    corpus_texts = []\n",
    "    for doc in corpus_data[\"document\"]:\n",
    "        for utterance in doc.get(\"utterance\", []):\n",
    "            if \"corrected_form\" in utterance:\n",
    "                corpus_texts.append(utterance[\"corrected_form\"])  # 교정된 문장만 추출\n",
    "    corpus_texts\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "    dictionary_path = \"./processing data/symspell_dictionary.txt\" #말뭉치에서 추출한 단어 빈도 기반으로 사용자 사전 구축\n",
    "    if not os.path.exists(dictionary_path):\n",
    "        with open(dictionary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence in corpus_texts:\n",
    "                words = sentence.split()\n",
    "                for word in words:\n",
    "                    f.write(f\"{word} 1\\n\")\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1) # SymSpell 사전 로드\n",
    "    \n",
    "    def symspell(row): # SymSpell을 활용한 오타 교정\n",
    "        content = row.content\n",
    "        corrected = sym_spell.lookup(content, Verbosity.CLOSEST, max_edit_distance=2) # SymSpell로 교정된 문장 찾기 (없으면 원래 문장 유지)\n",
    "        corrected_text = corrected[0].term if corrected else content\n",
    "        return corrected_text\n",
    "\n",
    "    data.content = data.apply(lambda x : symspell(x), axis = 1)   # 오타 수정 적용\n",
    "\n",
    "    def correct_text(row): #고쳐지지 않은 부분 수동으로 오타 수정\n",
    "        text = row.content\n",
    "        text = re.sub(r'올\\b', '을', text)\n",
    "        text = re.sub(r'틀\\b', '를', text)\n",
    "        text = re.sub(r'틀\\b', '를', text)\n",
    "        text = re.sub(r'논\\b', '는', text)\n",
    "        text = re.sub(r'않 듣다\\b', '않는다', text)\n",
    "        text = re.sub(r'즉정\\b', '측정', text)\n",
    "        text = re.sub(r'양 논다\\b', '않는다', text)\n",
    "        text = re.sub(r'하분지름', '하는지를', text)\n",
    "        text = re.sub(r'인플레이선', '인플레이션', text)\n",
    "        return text\n",
    "    \n",
    "    data.content = data.apply(lambda x : correct_text(x), axis = 1) # 조사 수정 적용\n",
    "    \n",
    "    def process_text(text):\n",
    "        model = BartForConditionalGeneration.from_pretrained(\"hsyooooo/kobart_spellcheck\").to(DEVICE)\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(\"hsyooooo/kobart_spellcheck\")\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, truncation=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_length=256, num_beams=2, early_stopping=True)\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    data[\"content\"] = data[\"content\"].apply(lambda x: process_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "    def correct_text(row):\n",
    "        corrections = {\n",
    "            \"지포로는\": \"지표로는\",\n",
    "            \"다른다:\": \"다룬다.\",\n",
    "            \"앞는\": \"않는\",\n",
    "            \"즉정할\": \"측정할\",\n",
    "            \"시장울\": \"시장을\",\n",
    "            \"변화울을\": \"변화율을\",\n",
    "            \"아분\": \"아닌\",\n",
    "            \"원직적으로\": \"원칙적으로\",\n",
    "            \"7년을\": \"1년을\",\n",
    "            \"꾸즈네즈가\": \"쿠즈네츠가\",\n",
    "            \"뉴필\":\"뉴딜\",\n",
    "            \"자유 거래\": \"자유거래\",\n",
    "            \"경 제학의\": \"경제학의\",\n",
    "            \"실업률 경제활동인구에서\": \"실업률은 경제활동인구에서\",\n",
    "        }\n",
    "        text = row[\"content\"] if isinstance(row[\"content\"], str) else \"\"  # 문자열이 아닐 경우 빈 문자열로 처리\n",
    "        for wrong, correct in corrections.items():\n",
    "            text = re.sub(rf'\\b{wrong}\\b', correct, text)  # 단어 경계를 고려하여 치환\n",
    "        return text\n",
    "\n",
    "    data[\"content\"] = data.apply(lambda x: correct_text(x), axis=1) # text_data의 \"content\" 컬럼에 적용\n",
    "    data = data.drop(index=[5, 6, 8, 15, 18, 22]).reset_index(drop=True) # 데이터프레임에서 해당 인덱스 삭제\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 저장된 불용어 리스트 불러오기\n",
    "stop_words_path = \"./processing data/stopwords-ko.txt\"\n",
    "with open(\"./processing data/stopwords-ko.txt\", 'r', encoding='utf-8') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 텍스트 전처리 클래스\n",
    "class nlp_before_qg:\n",
    "    \n",
    "    def __init__(self, cv_df):\n",
    "        page_num = str(input(\"원하는 페이지 번호 입력(ex: 5 혹은 1-4 혹은 all): \")) # 원하는 페이지 번호 입력 (예: 5, 1-4, all)\n",
    "        self.cv_df = cv_df\n",
    "        self.page_num = page_num\n",
    "        self.raw_text = None\n",
    "        self.text = None\n",
    "        self.text_list = None\n",
    "        self.text_summary = None\n",
    "        self.con_split = None\n",
    "        self.key_word = None\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 원본 텍스트 추출 함수: 지정된 페이지 번호에 해당하는 텍스트 추출\n",
    "    def raw_text_extract(self):\n",
    "        df = self.cv_df.copy()\n",
    "        page_num = self.page_num\n",
    "        if page_num == 'all':\n",
    "            df = df.loc[df['region_type'] == '평문', ['page_number', 'region_type', 'content']] # 모든 페이지에서 '평문' 영역 텍스트 추출\n",
    "            df.loc[:, 'content'] = df['content'].str.replace(':', '.')\n",
    "            raw_text = df['content'].to_list()\n",
    "        else:\n",
    "            start, end = (map(int, page_num.split('-')) if \"-\" in page_num else (int(page_num), int(page_num))) # 특정 페이지 범위에서 '평문' 영역 텍스트 추출\n",
    "            end += 1\n",
    "            page_list = range(start, end, 1)\n",
    "            df = df.loc[df['region_type'] == '평문', ['page_number', 'region_type', 'content']]\n",
    "            df['content'] = df['content'].str.replace('다:', '다.')\n",
    "            raw_text = df.loc[df['page_number'].isin(page_list), 'content'].to_list()\n",
    "        self.raw_text = raw_text\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 불필요한 문자 및 개행 문자 제거 함수\n",
    "    def text_resub(self):\n",
    "        text_list = self.raw_text\n",
    "        text = []\n",
    "        for sent in tqdm(text_list, desc=\"불필요한 부분 제거\", unit=\"sentences\"):\n",
    "            sent = re.sub('[^a-zA-Zㄱ-ㅣ가-힣0-9.\\s()]', '', sent)\n",
    "            sent = re.sub('[\\n]', '', sent)\n",
    "            text.append(sent)\n",
    "        self.text = \" \".join(text)\n",
    "        print(\"불필요한 부분 제거 완료\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 문장 분리 함수: KSS 라이브러리를 사용하여 문장 단위로 분리\n",
    "    def split_sentence(self):\n",
    "        self.text_list = kss.split_sentences(self.text)\n",
    "        print(\"문장 분리 완료\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 텍스트 요약 함수: KoT5 모델을 사용하여 요약 수행\n",
    "    def summary_sentence(self):\n",
    "        text = self.text_list\n",
    "        device_t = 0 if DEVICE == \"cuda\" else -1\n",
    "        model_name = \"psyche/KoT5-summarization\"\n",
    "        summarizer = pipeline(\"summarization\", model=model_name, device=device_t)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        summary_list = []\n",
    "        for t in tqdm(text, desc=\"요약 실행\", unit=\"sentences\"):\n",
    "            tokens = tokenizer.encode(t, return_tensors=\"pt\")\n",
    "            max_length = int(len(tokens[0]))\n",
    "            min_length = 30 if max_length >= 30 else max_length\n",
    "            summary = summarizer(t, min_length=min_length, max_length=max_length, do_sample=False)[0][\"summary_text\"]\n",
    "            summary_list.append(summary)\n",
    "        self.text_summary = summary_list\n",
    "        print(\"요약 완료\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 문맥 구분 함수: BERT NSP 모델을 사용하여 문맥 자동 분할\n",
    "    def context_split(self):\n",
    "        sentences = self.text_summary\n",
    "        model_path = '0Kyung/KoBERT-NextSentencePrediction' # NSP 모델 로드\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        model = BertForNextSentencePrediction.from_pretrained(model_path).to(DEVICE).eval()\n",
    "        transition_adverbs = [\"그러나\", \"하지만\", \"반면\", \"반대로\", \"달리\", \"불구하고\", \"그럼에도\"] # 연결 부사를 이용한 가중치 조정\n",
    "\n",
    "        def get_nsp_score(sent1, sent2):\n",
    "            tokens = tokenizer(sent1, sent2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            tokens = {key: value.to(DEVICE) for key, value in tokens.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokens)\n",
    "            logits = outputs.logits\n",
    "            return torch.softmax(logits, dim=1)[:, 0].item()\n",
    "\n",
    "        def calculate_weighted_similarity(sent1, sent2):\n",
    "            score = get_nsp_score(sent1, sent2)\n",
    "            return score + (0.1 if any(adverb in sent2 for adverb in transition_adverbs) else 0)\n",
    "\n",
    "        nsp_scores = [calculate_weighted_similarity(sentences[i], sentences[i+1]) for i in range(len(sentences)-1)]\n",
    "        threshold = np.mean(nsp_scores) + np.std(nsp_scores)  # 이상치 탐지를 통한 문맥 분할\n",
    "        change_points = [i for i, score in enumerate(nsp_scores) if score > threshold]\n",
    "        con_split, start = [], 0\n",
    "        for change in change_points:\n",
    "            con_split.append(\" \".join(sentences[start:change+1]))\n",
    "            start = change + 1\n",
    "        con_split.append(\" \".join(sentences[start:]))\n",
    "        self.con_split = con_split\n",
    "        print(\"문맥 구분 완료\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 키워드 추출 함수: KeyBERT를 사용하여 각 문맥별 핵심 키워드 추출\n",
    "    def keybert(self):\n",
    "        con_split = self.con_split\n",
    "    \n",
    "        def mmr(doc_embedding, candidate_embeddings, words, top_n=5, diversity=0.5):\n",
    "            word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "            word_similarity = cosine_similarity(candidate_embeddings)\n",
    "            keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "            candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "            for _ in range(top_n - 1):\n",
    "                if not candidates_idx:  # 후보가 없다면\n",
    "                    print(\"No more candidates left!\")\n",
    "                    break\n",
    "                candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "                target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "                mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "                if mmr.size == 0:  # mmr이 비어 있다면\n",
    "                    print(\"MMR array is empty!\")\n",
    "                    break\n",
    "                mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "                keywords_idx.append(mmr_idx)\n",
    "                candidates_idx.remove(mmr_idx)\n",
    "            return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "        embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device = DEVICE)\n",
    "        kw_model = KeyBERT(embedding_model)\n",
    "        final = []\n",
    "        for text in tqdm(con_split, desc=\"키워드 추출\", unit=\"contexts\"):\n",
    "            if not text.strip():\n",
    "                final.append([])\n",
    "                continue\n",
    "            keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), use_maxsum=False, top_n=10)\n",
    "            candidate = [key[0] for key in keywords]\n",
    "            doc_embedding = embedding_model.encode([text])\n",
    "            candidate_embedding = embedding_model.encode(candidate)\n",
    "            key_result = mmr(doc_embedding, candidate_embedding, candidate, top_n=3, diversity=0.5)\n",
    "            final.append(key_result)\n",
    "        okt = Okt()\n",
    "        key_total = []\n",
    "        for word_list in tqdm(final, desc=\"불용어 처리\", unit=\"keywords\"):\n",
    "            if not word_list:\n",
    "                continue\n",
    "            key_final = []\n",
    "            for word in word_list:\n",
    "                nouns = okt.nouns(word)\n",
    "                for i in range(len(nouns)):\n",
    "                    if nouns[i] in stop_words:\n",
    "                        nouns[i] = ''\n",
    "                joined = \"\".join(nouns)\n",
    "                key_final.append(joined)\n",
    "            key_total.append(key_final)\n",
    "\n",
    "        self.key_word = key_total\n",
    "        print(\"키워드 추출 완료\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 최종 결과를 데이터프레임으로 변환\n",
    "    def merge_to_df(self):\n",
    "        return pd.DataFrame(zip(self.con_split, self.key_word), columns=['context', 'keyword'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON to context & keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# JSON 파일을 불러와 NLP 처리 후 저장하는 함수\n",
    "def json_processing():\n",
    "    # 1. JSON 데이터 로드\n",
    "    input_df = pd.read_json(\"./processing data/PDF2JSON/result.json\")\n",
    "    # 1. 그래프 처리\n",
    "    input_df = plot_to_text(input_df)\n",
    "    # 2. 표 처리\n",
    "    input_df = table_to_text(input_df)\n",
    "    # 3. 교정\n",
    "    input_df = spell_correction(input_df)\n",
    "    # 4. Processor 클래스를 사용하여 NLP 처리 수행\n",
    "    nbq = nlp_before_qg(input_df)\n",
    "    nbq.raw_text_extract()\n",
    "    nbq.text_resub()\n",
    "    nbq.split_sentence()\n",
    "    nbq.summary_sentence()\n",
    "    nbq.context_split()\n",
    "    nbq.keybert()\n",
    "    new_df = nbq.merge_to_df()\n",
    "    # 5. 저장\n",
    "    final_path = os.path.join('./processing data/JSON2JSON', 'result.json')\n",
    "    directory = os.path.dirname(final_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \"\"\" 이미 저장된 JSON 파일이 있다면 덮어쓰기 할 것인지 확인\n",
    "    if os.path.exists(final_path):\n",
    "        user_input = input(f\"File {final_path} already exists. Overwrite? (y/n): \").lower()\n",
    "        if user_input != 'y':\n",
    "            print(f\"Skipping {final_path}\")\n",
    "            return\"\"\"\n",
    "    new_df.to_json(final_path, orient='records', force_ascii=False, indent=4)\n",
    "    torch.cuda.empty_cache()  # GPU 캐시 메모리 해제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 주관식 문제 생성 함수\n",
    "def generate_short(model, tokenizer, context, answer):\n",
    "    input_text = f\"질문 생성: {answer} 문맥: {context}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "    with torch.no_grad(): outputs = model.generate(input_ids, max_length=40, num_beams=3, repetition_penalty=1.5, no_repeat_ngram_size=2, temperature=1.0, top_k=30, top_p=0.8, early_stopping=True)\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    question = re.sub(r\"\\?.*\", \"?\", question).strip()\n",
    "    return question\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# ox 문제 생성 함수\n",
    "def generate_ox(model, tokenizer, context, ox):\n",
    "    ox_str = \"True\" if ox else \"False\"\n",
    "    input_text = f\"context: {context}\\n ox: {ox_str}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "    with torch.no_grad(): output_ids = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=25, do_sample=True, top_k=50, top_p=0.9, repetition_penalty=3.0, no_repeat_ngram_size=2, num_beams=10, early_stopping=True)\n",
    "    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    question = re.sub(r'다(\\.다)+\\.', '다.', question)\n",
    "    question = re.sub(r'\\.\\.+', '.', question)\n",
    "    question = re.sub(r'(?<=다\\.)(?=[^\\s])', ' ', question)\n",
    "    question = question.strip()\n",
    "    last_period_index = question.rfind('.')\n",
    "    if last_period_index:\n",
    "        question = question[:last_period_index+1]\n",
    "    return question\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 빈칸 문제 생성 함수\n",
    "def generate_blank(context, keyword):\n",
    "    return context.replace(keyword, \"[\"+len(keyword)*\" \"+\"]\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 서술형 문제 생성 함수\n",
    "def generate_essay(keyword):\n",
    "    return keyword + \"에 대해 서술하시오.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 주관식 문제 모델 로드\n",
    "shrot_tokenizer = AutoTokenizer.from_pretrained(\"PG18/Generate_shortanswer_question\")\n",
    "shrot_model = AutoModelForSeq2SeqLM.from_pretrained(\"PG18/Generate_shortanswer_question\").to(DEVICE)\n",
    "shrot_model.eval()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# ox 문제 모델 로드\n",
    "ox_tokenizer = AutoTokenizer.from_pretrained(\"asteroidddd/kobart-oxquiz\")\n",
    "ox_model = AutoModelForSeq2SeqLM.from_pretrained(\"asteroidddd/kobart-oxquiz\").to(DEVICE)\n",
    "ox_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퀴즈 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 퀴즈 생성 함수\n",
    "def make_quiz():\n",
    "    # 1. 데이터 로드\n",
    "    data_path = \"./processing data/JSON2JSON/result.json\"\n",
    "    data = pd.read_json(data_path)\n",
    "    # 2. 생성 문제 저장 데이터프레임\n",
    "    generation_data = pd.DataFrame(columns=[\"문제\", \"답\", \"유형\"])\n",
    "    # 3. 원하는 문제 선택\n",
    "    while True:\n",
    "        print(\"원하는 문제 종류를 선택하세요\\n\\t1. 주관식 문제\\n\\t2. OX 문제\\n\\t3. 빈칸 문제\\n\\t4. 서술형 문제\\n원하는 유형을 띄어쓰기로 구분하세요 (ex. 1 2 4)\")\n",
    "        try:\n",
    "            num = list(map(int, input().split()))\n",
    "            if all(1 <= n <= 4 for n in num):  # 입력값 검증\n",
    "                break\n",
    "        except:\n",
    "            pass  # 예외 발생 시 다시 입력\n",
    "        clear_output()\n",
    "        print(\"다시 입력하세요\")\n",
    "    # 4. 원하는 문제 생성\n",
    "    for n in num:\n",
    "        if n == 1:  # 주관식 문제 생성\n",
    "            for context, keywords in zip(data[\"context\"], data[\"keyword\"]):\n",
    "                if not isinstance(keywords, list):\n",
    "                    continue\n",
    "                for keyword in keywords:\n",
    "                    if not keyword.strip():\n",
    "                        continue\n",
    "                    answer = keyword\n",
    "                    question = generate_short(shrot_model, shrot_tokenizer, context, answer)\n",
    "                    new_row = pd.DataFrame([{\"문제\": question, \"답\": answer, \"유형\": \"주관식\"}])\n",
    "                    generation_data = pd.concat([generation_data, new_row], ignore_index=True)\n",
    "        elif n == 2:  # OX 문제 생성\n",
    "            all_text = \" \".join([str(context) for context in data[\"context\"] if isinstance(context, str)])\n",
    "            sentences = [s.strip() + \"다.\" for s in all_text.split(\"다.\") if s.strip()]\n",
    "            if not sentences:\n",
    "                continue\n",
    "            else:\n",
    "                for sentence in sentences:\n",
    "                    answer = random.choice([True, False])\n",
    "                    question = generate_ox(ox_model, ox_tokenizer, sentence, answer)\n",
    "                    if len(question) < 3:\n",
    "                        continue\n",
    "                    new_row = pd.DataFrame([{\"문제\": question, \"답\": answer, \"유형\": \"OX\"}])\n",
    "                    generation_data = pd.concat([generation_data, new_row], ignore_index=True)\n",
    "        elif n == 3: # 빈칸 문제 생성\n",
    "            for context, keywords in zip(data[\"context\"], data[\"keyword\"]):\n",
    "                if not isinstance(keywords, list):\n",
    "                    continue\n",
    "                for keyword in keywords:\n",
    "                    if not keyword.strip():\n",
    "                        continue\n",
    "                    answer = keyword\n",
    "                    question = generate_blank(context, answer)\n",
    "                    new_row = pd.DataFrame([{\"문제\": question, \"답\": answer, \"유형\": \"빈칸\"}])\n",
    "                    generation_data = pd.concat([generation_data, new_row], ignore_index=True)\n",
    "        elif n == 4:  # 서술형 문제 생성\n",
    "            unique_keywords = set()\n",
    "            for keywords in data[\"keyword\"]:\n",
    "                if isinstance(keywords, list):\n",
    "                    unique_keywords.update([kw.strip() for kw in keywords if kw.strip()])\n",
    "            for keyword in unique_keywords:\n",
    "                question = generate_essay(keyword)\n",
    "                answer = \"--\"\n",
    "                new_row = pd.DataFrame([{\"문제\": question, \"답\": answer, \"유형\": \"서술형\"}])\n",
    "                generation_data = pd.concat([generation_data, new_row], ignore_index=True)\n",
    "    # 5. 문제 저장\n",
    "    saving_dir = \"quiz_data.csv\"\n",
    "    generation_data.to_csv(saving_dir, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2json()\n",
    "json_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_quiz()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bitamin_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
