# -*- coding: utf-8 -*-
"""Improved _train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HV7DwP7mKm6p2dND4DQZoITr690PdND
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets

!ls /content/drive/MyDrive/BITAMIN/gen_question/requirements.txt
!pip install -r /content/drive/MyDrive/BITAMIN/gen_question/requirements.txt

!pip freeze > ../requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/BITAMIN/gen_question/src

import torch
from torch.utils.data import DataLoader
from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util
from datasets import load_dataset
from dataset import get_datasets
from model import load_model
import random

# ✅ 모델 및 토크나이저 로드
MODEL_PATH = "../saved_model/kobart_qg_finetuned"
tokenizer, model = load_model(MODEL_PATH)
model.to("cuda")

# ✅ 유사도 평가 모델 (문장 임베딩용)
reward_model = SentenceTransformer("BM-K/KoSimCSE-roberta")
import torch
from torch.utils.data import DataLoader
from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util
from datasets import load_dataset
from dataset import get_datasets
from model import load_model
import random

# ✅ 모델 및 토크나이저 로드
MODEL_PATH = "../saved_model/kobart_qg_finetuned"
tokenizer, model = load_model(MODEL_PATH)
model.to("cuda")

# ✅ 유사도 평가 모델 (문장 임베딩용)
reward_model = SentenceTransformer("BM-K/KoSimCSE-roberta")

# ✅ 데이터 로드
train_dataset, valid_dataset = get_datasets()  # 🔥 데이터셋 직접 설정

# ✅ 랜덤 샘플링 (Train: 1000개, Valid: 200개)
TRAIN_SAMPLE_SIZE = 1000
VALID_SAMPLE_SIZE = 200

train_indices = random.sample(range(len(train_dataset)), min(len(train_dataset), TRAIN_SAMPLE_SIZE))
valid_indices = random.sample(range(len(valid_dataset)), min(len(valid_dataset), VALID_SAMPLE_SIZE))

train_dataset = train_dataset.select(train_indices)
valid_dataset = valid_dataset.select(valid_indices)
# ✅ 데이터 전처리 함수
def preprocess_function(examples):
    """데이터를 모델 학습용으로 변환"""
    inputs = tokenizer(
        examples["input_text"],  # context + answer
        max_length=512,
        padding="max_length",
        truncation=True
    )

    labels = tokenizer(
        examples["target_text"],  # 정답(question)
        max_length=128,
        padding="max_length",
        truncation=True
    )

    inputs["labels"] = labels["input_ids"]
    return inputs

# ✅ 데이터 토크나이징
train_tokenized = train_dataset.map(preprocess_function, batched=True)
valid_tokenized = valid_dataset.map(preprocess_function, batched=True)

# ✅ 필요한 컬럼만 남기기
columns_to_keep = ["input_ids", "attention_mask", "labels"]
train_tokenized = train_tokenized.remove_columns(set(train_tokenized.column_names) - set(columns_to_keep))
valid_tokenized = valid_tokenized.remove_columns(set(valid_tokenized.column_names) - set(columns_to_keep))

train_tokenized.set_format(type="torch", columns=columns_to_keep)
valid_tokenized.set_format(type="torch", columns=columns_to_keep)

# ✅ DataLoader 설정
batch_size = 8
train_dataloader = DataLoader(train_tokenized, batch_size=batch_size, shuffle=True)
valid_dataloader = DataLoader(valid_tokenized, batch_size=batch_size, shuffle=False)

print(f"✅ 기존 데이터셋 준비 완료! 학습 샘플 수: {len(train_tokenized)}")

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="../saved_model/qg_supervised",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,  # 🔥 조절 가능
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    predict_with_generate=True  # ✅ `generate()` 호출 가능하도록 설정
)

class CustomTrainer(Seq2SeqTrainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # ✅ **kwargs 추가!
        """
        문장 유사도를 기반으로 Loss를 계산하는 커스텀 Trainer.
        """
        outputs = model(**inputs)
        logits = outputs.logits

        # ✅ 정답(question) 디코딩
        labels = inputs.get("labels")
        if labels is None:  # 만약 labels가 없으면 손실 계산을 건너뜀
            return outputs.loss if return_outputs else outputs.loss.mean()

        generated_questions = tokenizer.batch_decode(torch.argmax(logits, dim=-1), skip_special_tokens=True)
        reference_questions = tokenizer.batch_decode(labels, skip_special_tokens=True)

        # ✅ 문장 유사도 기반 Loss 계산
        losses = []
        for g, r in zip(generated_questions, reference_questions):
            g_emb = reward_model.encode(g, convert_to_tensor=True)
            r_emb = reward_model.encode(r, convert_to_tensor=True)
            similarity = util.pytorch_cos_sim(g_emb, r_emb).item()
            loss = 1 - similarity  # 유사할수록 Loss 낮음
            losses.append(loss)

        loss = torch.tensor(losses, requires_grad=True).mean().to("cuda")  # ✅ GPU로 이동!
        return (loss, outputs) if return_outputs else loss

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=valid_tokenized,
    tokenizer=tokenizer
)

# ✅ 학습 실행
trainer.train()

# ✅ 모델 저장
model.save_pretrained("../saved_model/qg_supervised")
tokenizer.save_pretrained("../saved_model/qg_supervised")

