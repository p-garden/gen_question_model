# -*- coding: utf-8 -*-
"""Improved _train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HV7DwP7mKm6p2dND4DQZoITr690PdND
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets

!ls /content/drive/MyDrive/BITAMIN/gen_question/requirements.txt
!pip install -r /content/drive/MyDrive/BITAMIN/gen_question/requirements.txt

!pip freeze > ../requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/BITAMIN/gen_question/src

import torch
from torch.utils.data import DataLoader
from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util
from datasets import load_dataset
from dataset import get_datasets
from model import load_model
import random

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "../saved_model/kobart_qg_finetuned"
tokenizer, model = load_model(MODEL_PATH)
model.to("cuda")

# âœ… ìœ ì‚¬ë„ í‰ê°€ ëª¨ë¸ (ë¬¸ì¥ ì„ë² ë”©ìš©)
reward_model = SentenceTransformer("BM-K/KoSimCSE-roberta")
import torch
from torch.utils.data import DataLoader
from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util
from datasets import load_dataset
from dataset import get_datasets
from model import load_model
import random

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "../saved_model/kobart_qg_finetuned"
tokenizer, model = load_model(MODEL_PATH)
model.to("cuda")

# âœ… ìœ ì‚¬ë„ í‰ê°€ ëª¨ë¸ (ë¬¸ì¥ ì„ë² ë”©ìš©)
reward_model = SentenceTransformer("BM-K/KoSimCSE-roberta")

# âœ… ë°ì´í„° ë¡œë“œ
train_dataset, valid_dataset = get_datasets()  # ğŸ”¥ ë°ì´í„°ì…‹ ì§ì ‘ ì„¤ì •

# âœ… ëœë¤ ìƒ˜í”Œë§ (Train: 1000ê°œ, Valid: 200ê°œ)
TRAIN_SAMPLE_SIZE = 1000
VALID_SAMPLE_SIZE = 200

train_indices = random.sample(range(len(train_dataset)), min(len(train_dataset), TRAIN_SAMPLE_SIZE))
valid_indices = random.sample(range(len(valid_dataset)), min(len(valid_dataset), VALID_SAMPLE_SIZE))

train_dataset = train_dataset.select(train_indices)
valid_dataset = valid_dataset.select(valid_indices)
# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    """ë°ì´í„°ë¥¼ ëª¨ë¸ í•™ìŠµìš©ìœ¼ë¡œ ë³€í™˜"""
    inputs = tokenizer(
        examples["input_text"],  # context + answer
        max_length=512,
        padding="max_length",
        truncation=True
    )

    labels = tokenizer(
        examples["target_text"],  # ì •ë‹µ(question)
        max_length=128,
        padding="max_length",
        truncation=True
    )

    inputs["labels"] = labels["input_ids"]
    return inputs

# âœ… ë°ì´í„° í† í¬ë‚˜ì´ì§•
train_tokenized = train_dataset.map(preprocess_function, batched=True)
valid_tokenized = valid_dataset.map(preprocess_function, batched=True)

# âœ… í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°
columns_to_keep = ["input_ids", "attention_mask", "labels"]
train_tokenized = train_tokenized.remove_columns(set(train_tokenized.column_names) - set(columns_to_keep))
valid_tokenized = valid_tokenized.remove_columns(set(valid_tokenized.column_names) - set(columns_to_keep))

train_tokenized.set_format(type="torch", columns=columns_to_keep)
valid_tokenized.set_format(type="torch", columns=columns_to_keep)

# âœ… DataLoader ì„¤ì •
batch_size = 8
train_dataloader = DataLoader(train_tokenized, batch_size=batch_size, shuffle=True)
valid_dataloader = DataLoader(valid_tokenized, batch_size=batch_size, shuffle=False)

print(f"âœ… ê¸°ì¡´ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_tokenized)}")

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="../saved_model/qg_supervised",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,  # ğŸ”¥ ì¡°ì ˆ ê°€ëŠ¥
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2,
    predict_with_generate=True  # âœ… `generate()` í˜¸ì¶œ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
)

class CustomTrainer(Seq2SeqTrainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # âœ… **kwargs ì¶”ê°€!
        """
        ë¬¸ì¥ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Lossë¥¼ ê³„ì‚°í•˜ëŠ” ì»¤ìŠ¤í…€ Trainer.
        """
        outputs = model(**inputs)
        logits = outputs.logits

        # âœ… ì •ë‹µ(question) ë””ì½”ë”©
        labels = inputs.get("labels")
        if labels is None:  # ë§Œì•½ labelsê°€ ì—†ìœ¼ë©´ ì†ì‹¤ ê³„ì‚°ì„ ê±´ë„ˆëœ€
            return outputs.loss if return_outputs else outputs.loss.mean()

        generated_questions = tokenizer.batch_decode(torch.argmax(logits, dim=-1), skip_special_tokens=True)
        reference_questions = tokenizer.batch_decode(labels, skip_special_tokens=True)

        # âœ… ë¬¸ì¥ ìœ ì‚¬ë„ ê¸°ë°˜ Loss ê³„ì‚°
        losses = []
        for g, r in zip(generated_questions, reference_questions):
            g_emb = reward_model.encode(g, convert_to_tensor=True)
            r_emb = reward_model.encode(r, convert_to_tensor=True)
            similarity = util.pytorch_cos_sim(g_emb, r_emb).item()
            loss = 1 - similarity  # ìœ ì‚¬í• ìˆ˜ë¡ Loss ë‚®ìŒ
            losses.append(loss)

        loss = torch.tensor(losses, requires_grad=True).mean().to("cuda")  # âœ… GPUë¡œ ì´ë™!
        return (loss, outputs) if return_outputs else loss

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=valid_tokenized,
    tokenizer=tokenizer
)

# âœ… í•™ìŠµ ì‹¤í–‰
trainer.train()

# âœ… ëª¨ë¸ ì €ì¥
model.save_pretrained("../saved_model/qg_supervised")
tokenizer.save_pretrained("../saved_model/qg_supervised")

