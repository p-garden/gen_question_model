# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.6
#   kernelspec:
#     display_name: Python (gen_que_env)
#     language: python
#     name: gen_que_env
# ---

# +
from dataset import get_datasets
from model import load_model
from transformers import TrainingArguments, Trainer

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
tokenizer, model = load_model()
train_dataset, valid_dataset = get_datasets()  #  `dataset.py`ì—ì„œ ë³€í™˜ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

# ë°ì´í„° ì „ì²˜ë¦¬
def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["input_text"], 
        max_length=512, 
        padding="max_length", 
        truncation=True
    )
    
    labels = tokenizer(
        examples["target_text"], 
        max_length=128, 
        padding="max_length", 
        truncation=True
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_tokenized = train_dataset.map(preprocess_function, batched=True)
valid_tokenized = valid_dataset.map(preprocess_function, batched=True)


# +
# âœ… í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
training_args = TrainingArguments(
    output_dir="../saved_model/kobart_qg_finetuned",  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
     evaluation_strategy="steps",    # í‰ê°€ë¥¼ stepsë§ˆë‹¤ ì‹¤í–‰
    save_strategy="steps",          # ëª¨ë¸ ì €ì¥ë„ stepsë§ˆë‹¤ ì‹¤í–‰
    save_steps=500,                 # 500 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
    eval_steps=500,                 # 500 ìŠ¤í…ë§ˆë‹¤ í‰ê°€
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=100,
    load_best_model_at_end=True,    # ìµœì  ëª¨ë¸ ë¡œë“œ
)

# âœ… Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=valid_tokenized,
    tokenizer=tokenizer,
)

# âœ… í•™ìŠµ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    model.save_pretrained("../models/kobart_qg_finetuned")
    tokenizer.save_pretrained("../models/kobart_qg_finetuned")
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!")
# -


