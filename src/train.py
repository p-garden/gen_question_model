# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KIXYTVwBwIbvmoZC0W9VnvJzLYTBt-pl

ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •
"""

!pip install import_ipynb

!pip install nbimporter

!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/BITAMIN/gen_question/requirements.txt
!pip install -r /content/drive/MyDrive/BITAMIN/gen_question/requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/BITAMIN/gen_question/src

def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["input_text"],
        max_length=512,
        padding="max_length",
        truncation=True
    )

    labels = tokenizer(
        examples["target_text"],
        max_length=128,
        padding="max_length",
        truncation=True
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

from dataset import get_datasets
from model import load_model
from transformers import TrainingArguments, Trainer

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
tokenizer, model = load_model()
train_dataset, valid_dataset = get_datasets()  #  `dataset.py`ì—ì„œ ë³€í™˜ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

TRAIN_SAMPLE_SIZE = 800  # í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜
VALID_SAMPLE_SIZE = 200  # ê²€ì¦ ë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜

train_dataset = train_dataset.shuffle(seed=42).select(range(min(len(train_dataset), TRAIN_SAMPLE_SIZE)))
valid_dataset = valid_dataset.shuffle(seed=42).select(range(min(len(valid_dataset), VALID_SAMPLE_SIZE)))

print(f"âœ… í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}ê°œ")
print(f"âœ… ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(valid_dataset)}ê°œ")

# ë°ì´í„° ì „ì²˜ë¦¬ preprocess í•¨ìˆ˜ ì‚¬ìš©
train_tokenized = train_dataset.map(preprocess_function, batched=True)
valid_tokenized = valid_dataset.map(preprocess_function, batched=True)

# âœ… í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
training_args = TrainingArguments(
    output_dir="../saved_model/kobart_qg_finetuned",  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    evaluation_strategy="steps",    # í‰ê°€ë¥¼ stepsë§ˆë‹¤ ì‹¤í–‰
    save_strategy="steps",          # ëª¨ë¸ ì €ì¥ë„ stepsë§ˆë‹¤ ì‹¤|í–‰
    save_steps=500,                 # 500 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
    eval_steps=500,                 # 500 ìŠ¤í…ë§ˆë‹¤ í‰ê°€
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=100,
    load_best_model_at_end=True,    # ìµœì  ëª¨ë¸ ë¡œë“œ
)

# âœ… Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=valid_tokenized,
    tokenizer=tokenizer,
)

# âœ… í•™ìŠµ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    trainer.train()

    # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    model.save_pretrained("../saved_model/kobart_qg_finetuned")
    tokenizer.save_pretrained("../saved_model/kobart_qg_finetuned")
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!")

"""ì¶”ê°€í•™ìŠµ ì½”ë“œ / 1000ê°œ ë‹¨ìœ„ë¡œ í•™ìŠµí•  ì˜ˆì •(random data)"""

import import_ipynb
import nbimporter

from dataset import get_datasets
from model import load_model
from transformers import TrainingArguments, Trainer
import torch
import os


# âœ… ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ì¶”ê°€ í•™ìŠµ)
MODEL_PATH = "../saved_model/kobart_qg_finetuned"  # ì´ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
tokenizer, model = load_model(MODEL_PATH)  # ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
train_dataset, valid_dataset = get_datasets()  # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë¡œë“œ

# âœ… ì¶”ê°€ í•™ìŠµ ì‹œ ë°ì´í„° ì¼ë¶€ë§Œ ì‚¬ìš© (ìƒ˜í”Œë§)
TRAIN_SAMPLE_SIZE = 10  # í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜
VALID_SAMPLE_SIZE = 2 # ê²€ì¦ ë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜

train_dataset = train_dataset.shuffle().select(range(min(len(train_dataset), TRAIN_SAMPLE_SIZE)))
valid_dataset = valid_dataset.shuffle().select(range(min(len(valid_dataset), VALID_SAMPLE_SIZE)))

print(f"âœ… ì¶”ê°€ í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}ê°œ")
print(f"âœ… ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(valid_dataset)}ê°œ")

train_tokenized = train_dataset.map(preprocess_function, batched=True)
valid_tokenized = valid_dataset.map(preprocess_function, batched=True)

# âœ… ì¶”ê°€ í•™ìŠµì„ ìœ„í•œ TrainingArguments ì„¤ì •
training_args = TrainingArguments(
    output_dir="../saved_model/kobart_qg_finetuned",  # ìƒˆë¡œìš´ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    evaluation_strategy="steps",  # ë§¤ stepsë§ˆë‹¤ í‰ê°€
    save_strategy="steps",          # ëª¨ë¸ ì €ì¥ë„ stepsë§ˆë‹¤ ì‹¤|í–‰
    learning_rate=3e-5,
    logging_steps=1,
    disable_tqdm=False,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,  # ì¶”ê°€ í•™ìŠµ 3 Epoch
    save_steps=300,  # 300 ìŠ¤í…ë§ˆë‹¤ ëª¨ë¸ ì €ì¥
    save_total_limit=2,  # ìµœì‹  ëª¨ë¸ 2ê°œë§Œ ì €ì¥
    logging_dir="../logs",
    #logging_steps=100,  # 100 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    load_best_model_at_end=True,  # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    metric_for_best_model="eval_loss",
    resume_from_checkpoint=True,  # âœ… ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ
    report_to="none"  # wandb ë¡œê·¸ ì‚¬ìš© ì•ˆ í•¨

)

# âœ… Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=valid_tokenized,
    tokenizer=tokenizer,
)
os.environ["WANDB_DISABLED"] = "true"


# âœ… í•™ìŠµ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ ì¶”ê°€ í•™ìŠµ ì‹œì‘...")
    trainer.train()

    # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    model.save_pretrained("../saved_model/kobart_qg_finetuned", torch_dtype=torch.float16, safe_serialization=False)
    tokenizer.save_pretrained("../saved_model/kobart_qg_finetuned")
    print("âœ… ì¶”ê°€ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨!")

# âœ… ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì • (ì˜ˆ: checkpoint-1500)
CHECKPOINT_PATH = "../saved_model/kobart_qg_finetuned/checkpoint-900"

# âœ… ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer, model = load_model(CHECKPOINT_PATH)

model.save_pretrained("../saved_model/kobart_qg_finetuned", torch_dtype=torch.float16, safe_serialization=False)
tokenizer.save_pretrained("../saved_model/kobart_qg_finetuned")

!pip freeze > ../requirements.txt

!pip install --upgrade protobuf

import glob
import os

notebooks = glob.glob("*.ipynb")  # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  .ipynb íŒŒì¼ ì°¾ê¸°

for notebook in notebooks:
    py_file = notebook.replace(".ipynb", ".py")
    !jupyter nbconvert --to script {notebook}
    print(f"âœ… {py_file} ë³€í™˜ ì™„ë£Œ!")

# ë³€í™˜ëœ íŒŒì¼ì„ Google Driveì— ì €ì¥
drive_path = "/content/drive/MyDrive/BITAMIN/gen_question/src/"
os.makedirs(drive_path, exist_ok=True)
!mv *.py {drive_path}

print(f"ğŸ“‚ ëª¨ë“  .py íŒŒì¼ì´ {drive_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

