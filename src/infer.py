# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.6
#   kernelspec:
#     display_name: Python (gen_que_env)
#     language: python
#     name: gen_que_env
# ---

# +
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

# âœ… í•™ìŠµëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "../saved_model/kobart_qg_finetuned"  # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_PATH)
model = BartForConditionalGeneration.from_pretrained(MODEL_PATH)
model.eval()  # í‰ê°€ ëª¨ë“œ

def generate_question(context, answer):
    """
    ë¬¸ë§¥(context)ê³¼ ì •ë‹µ(answer)ì„ ì…ë ¥í•˜ë©´ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    input_text = f"ì§ˆë¬¸ ìƒì„±: {answer} ë¬¸ë§¥: {context}"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(model.device)

    # âœ… ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_length=50,
            num_beams=5,  # Beam Search ì ìš©
            repetition_penalty=2.5,  # ê°™ì€ í‘œí˜„ ë°˜ë³µ ë°©ì§€
            no_repeat_ngram_size=3,  # n-gram ë°˜ë³µ ë°©ì§€
            temperature=0.7,  # ìƒ˜í”Œë§ ë‹¤ì–‘ì„± ì¦ê°€
            top_k=50,  # í™•ë¥  ë†’ì€ 50ê°œ ì¤‘ ì„ íƒ
            top_p=0.9,  # í™•ë¥  ëˆ„ì  90% ë‚´ì—ì„œ ì„ íƒ
            early_stopping=True
        )

    # âœ… ìƒì„±ëœ ì§ˆë¬¸ ë””ì½”ë”©
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘!")

    test_data = [
        {
            "context": "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ë¡œ ì •ì¹˜, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ì´ë‹¤. ë˜í•œ í•œê°•ì´ íë¥´ë©° ì£¼ìš” í–‰ì • ê¸°ê´€ì´ ìœ„ì¹˜í•´ ìˆë‹¤.",
            "answer": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„"
        },
        {
            "context": "í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ëŠ” ì§ê°ì‚¼ê°í˜•ì—ì„œ ë¹—ë³€ì˜ ì œê³±ì´ ë‹¤ë¥¸ ë‘ ë³€ì˜ ì œê³±ì˜ í•©ê³¼ ê°™ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤.",
            "answer": "í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬"
        },
        {
            "context": "ì—ë””ìŠ¨ì€ ì „êµ¬ë¥¼ ë°œëª…í•œ ì¸ë¬¼ë¡œ ìœ ëª…í•˜ë©°, ê·¸ëŠ” ì „ê¸°ë¥¼ ì‹¤ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë° ê¸°ì—¬í•œ ë°œëª…ê°€ì´ë‹¤.",
            "answer": "ì „êµ¬"
        }
    ]

    for example in test_data:
        test_context = example["context"]
        test_answer = example["answer"]
        generated_question = generate_question(test_context, test_answer)

        print("\nğŸ¯ Context:", test_context)
        print("âœ… Generated Question:", generated_question)
        print("ğŸ“ Answer:", test_answer)
        print("-" * 80)

    print("ğŸš€ ì§ˆë¬¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

